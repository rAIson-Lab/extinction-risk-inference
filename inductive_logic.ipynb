{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae225fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random\n",
    "sys.path.append(os.path.join(os.getcwd(), 'CONFOLD')) #add CONFOLD to path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from foldrm import Classifier\n",
    "from utils import split_data # Or your stratified version if you prefer\n",
    "from ModifiedClass import  extinction_birds2 # Our new function\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b4c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce0f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = extinction_birds2()\n",
    "\n",
    "data_dummies = pd.get_dummies(data)\n",
    "\n",
    "numerical = ['Mass', 'HWI','Beak.Length.culmen','Beak.Length.nares',\n",
    "             'Beak.Width','Beak.Depth','Tarsus.Length','Wing.Length',\n",
    "             'Kipps.Distance','Secondary1','Tail.Length','LogRangeSize',\n",
    "             'LogClutchSize','LogNightLights','LogHumanPopulationDensity']\n",
    "\n",
    "#X = data.drop('Threat', axis=1).values.tolist()\n",
    "X = data[numerical].values.tolist()\n",
    "y = data['Threat'].values.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76798b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Classifier(attrs=model.attrs, numeric=model.numeric, label=model.label)\n",
    "\n",
    "train_data = np.concatenate((np.array(X_train), np.array(y_train).reshape(-1, 1)), axis=1).tolist()\n",
    "test_data = np.concatenate((np.array(X_test), np.array(y_test).reshape(-1, 1)), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b97b2f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Rules Learned by the Baseline Model ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Print the rules the model learned\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Rules Learned by the Baseline Model ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_asp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Prepare the test data (features and true labels)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m X_test \u001b[38;5;241m=\u001b[39m [d[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m test_data]\n",
      "File \u001b[0;32m~/repos/extinction-risk-inference/CONFOLD/foldrm.py:84\u001b[0m, in \u001b[0;36mClassifier.print_asp\u001b[0;34m(self, simple)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_asp\u001b[39m(\u001b[38;5;28mself\u001b[39m, simple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 84\u001b[0m     asp_rules_to_print \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m asp_rules_to_print:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28mprint\u001b[39m(r)\n",
      "File \u001b[0;32m~/repos/extinction-risk-inference/CONFOLD/foldrm.py:73\u001b[0m, in \u001b[0;36mClassifier.asp\u001b[0;34m(self, simple)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrs \u001b[38;5;241m=\u001b[39m [zip_rule(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrs]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simple:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masp_rules \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;241m=\u001b[39m add_constraint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrs)\n",
      "File \u001b[0;32m~/repos/extinction-risk-inference/CONFOLD/utils.py:403\u001b[0m, in \u001b[0;36mdecode_rules\u001b[0;34m(rules, attrs, x)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rule_str\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m rules:\n\u001b[0;32m--> 403\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_f2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_r\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/repos/extinction-risk-inference/CONFOLD/utils.py:381\u001b[0m, in \u001b[0;36mdecode_rules.<locals>._f2\u001b[0;34m(rule)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_f2\u001b[39m(rule):\n\u001b[0;32m--> 381\u001b[0m     head \u001b[38;5;241m=\u001b[39m \u001b[43m_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     body_elements \u001b[38;5;241m=\u001b[39m [ _f1(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(rule[\u001b[38;5;241m1\u001b[39m]) ]\n\u001b[1;32m    383\u001b[0m     tail_elements \u001b[38;5;241m=\u001b[39m [ _f1(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(rule[\u001b[38;5;241m2\u001b[39m]) ]\n",
      "File \u001b[0;32m~/repos/extinction-risk-inference/CONFOLD/utils.py:361\u001b[0m, in \u001b[0;36mdecode_rules.<locals>._f1\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    359\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m v \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m==\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(X,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m r \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m not_prefix \u001b[38;5;241m+\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(X,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m v \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "baseline_model.fit(train_data, ratio=0.5)\n",
    "# Print the rules the model learned\n",
    "print(\"--- Rules Learned by the Baseline Model ---\")\n",
    "baseline_model.print_asp(simple=True)\n",
    "\n",
    "# Prepare the test data (features and true labels)\n",
    "X_test = [d[:-1] for d in test_data]\n",
    "Y_test = [d[-1] for d in test_data]\n",
    "\n",
    "# Get predictions (these will be tuples of (label, confidence))\n",
    "predictions_tuples = baseline_model.predict(X_test)\n",
    "predicted_labels = [p[0] for p in predictions_tuples]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "for i in range(len(Y_test)):\n",
    "    if predicted_labels[i] == Y_test[i]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(Y_test)\n",
    "\n",
    "print(\"--- Baseline Model Evaluation ---\")\n",
    "print(f\"True Labels:    {Y_test}\")\n",
    "print(f\"Predicted Labels: {predicted_labels}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Instantiate a new classifier for our expert-guided model\n",
    "expert_model = Classifier(attrs=model_template.attrs, numeric=model_template.numeric, label=model_template.label)\n",
    "\n",
    "# Define our expert rules as strings\n",
    "# Note: the symbols '==' and '<=' must also be in single quotes for the parser.\n",
    "rule1 = \"with confidence 0.90 class = '1' if 'LogRangeSize' '<=' '5' except if 'Diet' '==' 'Generalist'\"\n",
    "#Note additional rules could be added like this:\n",
    "rule2 = \"with confidence 0.70 class = '1' if 'LogClutchSize' '==' '0'\"\n",
    "\n",
    "# Add the manual rules to the model\n",
    "expert_model.add_manual_rule(rule1, model_template.attrs, model_template.numeric, ['0', '1'], instructions=False)\n",
    "# Note: here is code to add an additional rule:\n",
    "expert_model.add_manual_rule(rule2, model_template.attrs, model_template.numeric, ['0', '1'], instructions=False)\n",
    "\n",
    "print(\"--- Manual Rules Added to the Model (Before Training) ---\")\n",
    "# The internal representation is a bit complex, but we can see our rules are in there.\n",
    "for rule in expert_model.rules:\n",
    "    print(rule)\n",
    "\n",
    "# Now, fit the model on the training data.\n",
    "# The algorithm will work around the rules we provided.\n",
    "expert_model.fit(train_data, ratio=0.75)\n",
    "\n",
    "# Print the final, combined rule set\n",
    "print(\"--- Final Ruleset from the Expert Model ---\")\n",
    "expert_model.print_asp(simple=True)\n",
    "\n",
    "# Get predictions from our new model\n",
    "expert_predictions_tuples = expert_model.predict(X_test)\n",
    "expert_predicted_labels = [p[0] for p in expert_predictions_tuples]\n",
    "\n",
    "# Calculate accuracy\n",
    "expert_correct_predictions = 0\n",
    "for i in range(len(Y_test)):\n",
    "    if expert_predicted_labels[i] == Y_test[i]:\n",
    "        expert_correct_predictions += 1\n",
    "\n",
    "expert_accuracy = expert_correct_predictions / len(Y_test)\n",
    "\n",
    "print(\"--- Baseline Model Evaluation ---\")\n",
    "print(f\"True Labels:      {Y_test}\")\n",
    "print(f\"Predicted Labels: {predicted_labels}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Expert Model Evaluation ---\")\n",
    "print(f\"True Labels:      {Y_test}\")\n",
    "print(f\"Predicted Labels: {expert_predicted_labels}\")\n",
    "print(f\"Accuracy: {expert_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Instantiate a new classifier\n",
    "learned_confidence_model = Classifier(attrs=model_template.attrs, numeric=model_template.numeric, label=model_template.label)\n",
    "\n",
    "# Define our expert rules as strings, but WITHOUT the 'with confidence' part.\n",
    "rule1_no_confidence = \"class = '1' if 'LogRangeSize' '<=' '5'\"\n",
    "rule2_no_confidence = \"class = '1' if 'LogClutchSize' '==' '0'\"\n",
    "\n",
    "# Add the manual rules to the model\n",
    "learned_confidence_model.add_manual_rule(rule1_no_confidence, model_template.attrs, model_template.numeric, ['0', '1'], instructions=False)\n",
    "learned_confidence_model.add_manual_rule(rule2_no_confidence, model_template.attrs, model_template.numeric, ['0', '1'], instructions=False)\n",
    "\n",
    "print(\"--- Manual Rules Added (Before Training) ---\")\n",
    "print(\"Notice the default confidence value of 0.5 assigned to each rule.\")\n",
    "for rule in learned_confidence_model.rules:\n",
    "    print(rule)\n",
    "\n",
    "# Now, fit the model on the training data.\n",
    "# The algorithm will calculate the confidence of our provided rules and then learn any additional rules needed.\n",
    "learned_confidence_model.fit(train_data, ratio=0.5)\n",
    "\n",
    "# Print the final, combined rule set\n",
    "print(\"--- Final Ruleset with Learned Confidence ---\")\n",
    "print(\"The confidence values have now been updated based on the training data!\")\n",
    "learned_confidence_model.print_asp(simple=True)\n",
    "            #Note that confidence values will be relatively low due to the small size of the training data. \n",
    "\n",
    "# Get predictions from our new model\n",
    "learned_conf_predictions = learned_confidence_model.predict(X_test)\n",
    "learned_conf_labels = [p[0] for p in learned_conf_predictions]\n",
    "\n",
    "# Calculate accuracy\n",
    "learned_conf_accuracy = sum(1 for i in range(len(Y_test)) if learned_conf_labels[i] == Y_test[i]) / len(Y_test)\n",
    "\n",
    "print(\"--- Learned Confidence Model Evaluation ---\")\n",
    "print(f\"True Labels:      {Y_test}\")\n",
    "print(f\"Predicted Labels: {learned_conf_labels}\")\n",
    "print(f\"Accuracy: {learned_conf_accuracy * 100:.2f}%\")\n",
    "\n",
    "# First, let's re-print the rules from our expert model for comparison\n",
    "print(\"--- Rules Before Pruning ---\")\n",
    "expert_model.print_asp(simple=True)\n",
    "\n",
    "############PRUNNING##################\n",
    "\n",
    "# Method 1: Simple Post-Hoc Confidence Pruning: removing those rules with a low confidence according to me\n",
    "# Import the prune_rules function from the core algorithm file\n",
    "from algo import prune_rules\n",
    "\n",
    "# Apply the pruning function\n",
    "# This will create a new list containing only the rules that meet the confidence threshold.\n",
    "pruned_rules = prune_rules(expert_model.rules, confidence=0.90)\n",
    "\n",
    "# We can create a new model instance to hold these pruned rules\n",
    "simple_pruned_model = Classifier(attrs=model_template.attrs, numeric=model_template.numeric, label=model_template.label)\n",
    "simple_pruned_model.rules = pruned_rules\n",
    "\n",
    "print(\"\\n--- Rules After Pruning (Confidence >= 0.90) ---\")\n",
    "simple_pruned_model.print_asp(simple=True)\n",
    "            \n",
    "# Instantiate a new model for this experiment\n",
    "advanced_pruning_model = Classifier(attrs=model_template.attrs, numeric=model_template.numeric, label=model_template.label)\n",
    "\n",
    "##################\n",
    "#### Method 2: Advanced Confidence-Driven Learning\n",
    "\n",
    "# Now, train using confidence_fit with a high 15% improvement threshold\n",
    "print(\"--- Training with confidence_fit(improvement_threshold=0.15) ---\")\n",
    "advanced_pruning_model.confidence_fit(train_data, improvement_threshold=0.15)\n",
    "\n",
    "print(\"\\n--- Rules Learned via Confidence-Driven Learning ---\")\n",
    "print(\"Note how the model is simpler and did not learn any exceptions to rules or `abnormalities', as they did not meet the high confidence improvement threshold.\")\n",
    "advanced_pruning_model.print_asp(simple=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilasp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
